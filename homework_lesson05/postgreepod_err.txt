Name:             postgres-bbd6d7c48-gfzkc
Namespace:        lesson05
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=postgres
                  pod-template-hash=bbd6d7c48
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container postgres; cpu, memory limit for container postgres
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/postgres-bbd6d7c48
Containers:
  postgres:
    Image:      postgres:10.13
    Port:       5432/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     100m
      memory:  64Mi
    Environment:
      POSTGRES_PASSWORD:  <set to the key 'POSTGRES_PASSWORD' in secret 'postgre-secret'>   Optional: false
      POSTGRES_DB:        <set to the key 'postgres_db' of config map 'postgres-config'>    Optional: false
      POSTGRES_USER:      <set to the key 'postgres_user' of config map 'postgres-config'>  Optional: false
      PGDATA:             <set to the key 'pgdata' of config map 'postgres-config'>         Optional: false
    Mounts:
      /var/lib/postgresql/data from pvc-nfs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6hrtc (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  pvc-nfs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc-nfs
    ReadOnly:   false
  kube-api-access-6hrtc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age                   From                Message
  ----     ------             ----                  ----                -------
  Warning  FailedScheduling   6m33s                 default-scheduler   0/2 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: True}, 1 node(s) had volume node affinity conflict. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..
  Warning  FailedScheduling   69s                   default-scheduler   0/2 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: True}, 1 node(s) had volume node affinity conflict. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..
  Normal   NotTriggerScaleUp  88s (x31 over 6m30s)  cluster-autoscaler  pod didn't trigger scale-up:
